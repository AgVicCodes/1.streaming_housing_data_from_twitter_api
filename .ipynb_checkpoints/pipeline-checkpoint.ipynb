{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5c26b93-d591-4d5f-a5bd-657a38fa9d1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twikit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtwikit\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'twikit'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import twikit\n",
    "import asyncio\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c42cc2e5-bc7e-4d53-9f04-2c580c4a56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "            'rent energy water tax Tv license groceries',\n",
    "            'Aberdeen rent energy water tax Tv license groceries',\n",
    "            'Bedford water tax groceries',\n",
    "            'Birmingham rent energy water tax Tv license groceries', \n",
    "            'Bolton rent energy water tax Tv license groceries',\n",
    "            'Bristol rent energy water tax Tv license groceries', \n",
    "            'Canterbury rent energy water tax Tv license groceries',\n",
    "            'Cambridgeshire rent energy water tax Tv license groceries',\n",
    "            'Coventry rent energy water tax Tv license groceries', \n",
    "            'Dartford rent energy water tax Tv license groceries',\n",
    "            'Derby rent energy water tax Tv license groceries',\n",
    "            'Dundee rent energy water tax Tv license groceries',\n",
    "            'Durham rent energy water tax Tv license groceries',\n",
    "            'Essex rent energy water tax Tv license groceries',\n",
    "            'Glasgow rent energy water tax Tv license groceries',\n",
    "            'Gloucester rent energy water tax Tv license groceries',\n",
    "            'Gosport rent energy water tax Tv license groceries',\n",
    "            'Ireland rent energy water tax Tv license groceries',\n",
    "            'Leeds rent energy water tax Tv license groceries',\n",
    "            'Leicester rent energy water tax Tv license groceries',\n",
    "            'Lincoln rent energy water tax Tv license groceries',\n",
    "            'London rent energy water tax Tv license groceries',\n",
    "            'Loughborough rent energy water tax Tv license groceries',\n",
    "            'Luton rent energy water tax Tv license groceries',\n",
    "            'Manchester rent energy water tax Tv license groceries',\n",
    "            'Middlesbrough rent energy water tax Tv license groceries',\n",
    "            'Northampton rent energy water tax Tv license groceries',\n",
    "            'Peterborough rent energy water tax Tv license groceries',\n",
    "            'Scotland rent energy water tax Tv license groceries',\n",
    "            'Sheffield rent energy water tax Tv license groceries',\n",
    "            'Stoke rent energy water tax Tv license groceries',\n",
    "            'Sunderland rent energy water tax Tv license groceries',\n",
    "            'Surrey rent energy water tax Tv license groceries'\n",
    "            'Swanley rent energy water tax Tv license groceries',\n",
    "            'Walsall rent energy water tax Tv license groceries',\n",
    "            'Westminster rent energy water tax Tv license groceries',\n",
    "            'Wolverhampton rent energy water tax Tv license groceries'\n",
    "        ]\n",
    "\n",
    "columns = [\"text\", \"username\", \"timestamp\", \"location\"]\n",
    "\n",
    "\n",
    "def extract_tweets():\n",
    "    \"\"\" \n",
    "    Twikit tweet extractor.\n",
    "\n",
    "    This function uses the keys in the keys.json file\n",
    "    to authenticate the twikit api and uses the search_tweet \n",
    "    attribute to extract tweets through the API.\n",
    "    Parameters\n",
    "\n",
    "    \"\"\"\n",
    "    with open('keys.json') as file:\n",
    "        keys = json.load(file)\n",
    "\n",
    "    # Creating a client instance\n",
    "    client = twikit.client.client.Client(language = \"en-US\")\n",
    "\n",
    "    # Defining an async function to handle the async login\n",
    "    async def main():\n",
    "        try:\n",
    "            await client.login(\n",
    "                auth_info_1 = keys['Email'],\n",
    "                auth_info_2 = keys['Username'],\n",
    "                password = keys['Password']\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Login failed: {e}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            client.save_cookies('cookies.json')\n",
    "        \n",
    "            with open('cookies.json', 'r', encoding='utf-8') as f:\n",
    "                client.set_cookies(json.load(f))\n",
    "\n",
    "            client.load_cookies('cookies.json')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't save cookies: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "        for i, query in enumerate(queries):\n",
    "\n",
    "            tweets = await client.search_tweet(query = query, product = \"Top\", count = 1000)\n",
    "\n",
    "            for tweet in tweets:\n",
    "                data = []\n",
    "                data.append([tweet.text, tweet.user.name, tweet.created_at, tweet.coordinates])\n",
    "                tweets_df = pd.DataFrame(data, columns = columns)\n",
    "\n",
    "            tweets_df.to_parquet(f\"data/parquet/tweets_df_{i}.parquet\", index = False)\n",
    "\n",
    "    # Running the async function using the event loop\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fda5846a-e010-48ae-a7c1-8ac83856dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv():\n",
    "    csv_files = glob.glob(f\"data/csv/*.{'csv'}\")\n",
    "\n",
    "    df_append = pd.DataFrame()\n",
    "\n",
    "    for csv in csv_files:\n",
    "        df_temp = pd.read_csv(csv)\n",
    "        df_append = df_append._append(df_temp, ignore_index = True)\n",
    "\n",
    "    df_append.to_parquet(\"data/parquet/twitter_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7202dcf-8cb4-42e1-aad6-0161199bb173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "\n",
    "    df = pd.read_parquet(\"data/parquet/twitter_df.parquet\", engine = \"pyarrow\")\n",
    "\n",
    "    df[\"text\"] = df[\"text\"].str.replace(\"\\n\", \" \")\n",
    "\n",
    "    df[\"county\"] = df[\"text\"].apply(lambda x: re.findall(county_regex, x)[0] if re.findall(county_regex, x) else np.nan)\n",
    "\n",
    "    df[\"rent_cost\"] = df[\"text\"].apply(lambda x: re.findall(rent_regex, x)[0] if re.findall(rent_regex, x) else np.nan)\n",
    "\n",
    "    df[\"no_of_rooms\"] = df[\"text\"].apply(lambda x: re.findall(room_regex, x)[0] if re.findall(room_regex, x) else np.nan)\n",
    "\n",
    "    df[\"energy_bill\"] = df[\"text\"].apply(lambda x: re.findall(energy_regex, x)[0] if re.findall(energy_regex, x) else np.nan)\n",
    "\n",
    "    df[\"council_tax\"] = df[\"text\"].apply(lambda x: re.findall(tax_regex, x)[0] if re.findall(tax_regex, x) else np.nan)\n",
    "\n",
    "    df[\"groceries\"] = df[\"text\"].apply(lambda x: re.findall(groceries_regex, x)[0] if re.findall(groceries_regex, x) else np.nan)\n",
    "\n",
    "    df[\"clothing\"] = df[\"text\"].apply(lambda x: re.findall(clothing_regex, x)[0] if re.findall(clothing_regex, x) else np.nan)  \n",
    "\n",
    "    df[\"water\"] = df[\"text\"].apply(lambda x: re.findall(water_regex, x)[0] if re.findall(water_regex, x) else np.nan)\n",
    "\n",
    "    new_df = df[[\"username\", \"county\", \"no_of_rooms\", \"rent_cost\", \"energy_bill\", \"water\", \"council_tax\", \"groceries\", \"clothing\"]]\n",
    "\n",
    "    new_df.to_parquet(\"data/parquet/regex_extracted_df.parquet\")\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "rent_regex = r\"(?:[Rr]ent|[Bb]edroom|[Bb]d|[Bb]ed|[Rr]oom\\s)\\s?[:-]?\\s?£?(\\d{1,2}?,?\\d{3,4})\"\n",
    "\n",
    "energy_regex = r\"[Ee]nergy\\s?[:-]*(?:\\D+\\s?)?\\s?£?(\\d?,?\\d+)+\"\n",
    "\n",
    "water_regex = r\"[Ww]ater\\s?[:-]*\\s?£?(\\d{1,3})\"\n",
    "\n",
    "tax_regex = r\"[Cc]ouncil\\s?[Tt]ax\\s?[:-]*\\s?£?(\\d{1,3})\"\n",
    "\n",
    "groceries_regex = r\"[Gg]roceries\\s?[:-]*\\s?£?(\\d{1,3})\"\n",
    "\n",
    "clothing_regex = r\"[Cc]lothing\\s?[:-]*\\s?£?(\\d{1,3})\"\n",
    "\n",
    "room_regex = r\"(\\d)\\s?(?:[Bb]edroom|[Bb]d|[Bb]ed|[Rr]oom)\"\n",
    "\n",
    "county_regex = r\"(?:Aberdeen|Bedford|Birmingham|Bolton|Bristol|Canterbury|Cambridgeshire|Coventry|Dartford|Derby|Dundee|Durham|Essex|Glasgow|Gloucester|Gosport|Ireland|Leeds|Leicester|Lincoln|London|Loughborough|Luton|Manchester|Middlesbrough|Northampton|Oxford|Peterborough|Scotland|Sheffield|Stoke|Sunderland|Surrey|Swanley|Walsall|Westminster|Wolverhampton)\\b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b2898d-a6cd-4932-b56b-4f141ca6e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_float(new_df):\n",
    "\n",
    "    new_df.loc[new_df[\"county\"].isna(), \"county\"] = \"United Kingdom\"\n",
    "\n",
    "    new_df.loc[:, \"rent_cost\"] = new_df[\"rent_cost\"].str.replace(\",\", \"\")\n",
    "    \n",
    "    new_df.loc[:, \"energy_bill\"] = new_df[\"energy_bill\"].str.replace(\",\", \"\")\n",
    "\n",
    "    new_df[\"rent_cost\"] = new_df[\"rent_cost\"].astype(\"float\")\n",
    "    \n",
    "    new_df[\"energy_bill\"] = new_df[\"energy_bill\"].astype(\"float\")\n",
    "    \n",
    "    new_df[\"water\"] = new_df[\"water\"].astype(\"float\")\n",
    "    \n",
    "    new_df[\"council_tax\"] = new_df[\"council_tax\"].astype(\"float\")\n",
    "    \n",
    "    new_df[\"groceries\"] = new_df[\"groceries\"].astype(\"float\")\n",
    "    \n",
    "    new_df[\"clothing\"] = new_df[\"clothing\"].astype(\"float\")\n",
    "\n",
    "    subset_df = new_df.copy()\n",
    "    \n",
    "    new_df.to_parquet(\"data/parquet/first_conversion_df.parquet\")\n",
    "\n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c923307-8a59-4a7b-afa1-6dcf75392d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_df(df):\n",
    "    \n",
    "    pseudo_rent = np.round(np.random.normal(df[\"rent_cost\"].mean(), 200, df[\"rent_cost\"].isnull().sum()), -2)\n",
    "\n",
    "    pseudo_energy = np.round(np.random.normal(df[\"energy_bill\"].mean(), 50, df[\"energy_bill\"].isnull().sum()), -1)\n",
    "\n",
    "    pseudo_water = np.round(np.random.normal(df[\"water\"].mean(), 5, df[\"water\"].isnull().sum()), -1)\n",
    "\n",
    "    pseudo_tax = np.round(np.random.normal(df[\"council_tax\"].mean(), 10, df[\"council_tax\"].isnull().sum()), -1)\n",
    "\n",
    "    pseudo_groceries = np.round(np.random.normal(df[\"groceries\"].mean(), 2, df[\"groceries\"].isnull().sum()), -1)\n",
    "\n",
    "    pseudo_clothing = np.round(np.random.normal(df[\"clothing\"].mean(), 10, df[\"clothing\"].isnull().sum()), -1)\n",
    "\n",
    "\n",
    "\n",
    "    df.loc[df[\"rent_cost\"].isna(), \"rent_cost\"] = pseudo_rent\n",
    "\n",
    "    df.loc[df[\"energy_bill\"].isna(), \"energy_bill\"] = pseudo_energy\n",
    "\n",
    "    df.loc[df[\"water\"].isna(), \"water\"] = pseudo_water\n",
    "\n",
    "    df.loc[df[\"council_tax\"].isna(), \"council_tax\"] = pseudo_tax\n",
    "\n",
    "    df.loc[df[\"groceries\"].isna(), \"groceries\"] = pseudo_groceries\n",
    "\n",
    "    df.loc[df[\"clothing\"].isna(), \"clothing\"] = pseudo_clothing\n",
    "\n",
    "\n",
    "\n",
    "    rent_range = [0, 600, 1000, np.inf]\n",
    "\n",
    "    room_map = [\"1\", \"2\", \"3\"]\n",
    "\n",
    "    df.loc[df[\"no_of_rooms\"].isna(), \"no_of_rooms\"] = pd.cut(df[\"rent_cost\"], bins = rent_range, labels = room_map)\n",
    "\n",
    "    df.loc[:, \"no_of_rooms\"] = df[\"no_of_rooms\"].astype(\"category\")\n",
    "\n",
    "    df.to_parquet(\"data/parquet/replaced_na_df.parquet\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ae5221c-7c6d-4d19-960a-4e96469ae017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_na_dup(subset_df):\n",
    "    \n",
    "    df = subset_df.copy()\n",
    "    \n",
    "    subset = [\"rent_cost\", \"energy_bill\"]\n",
    "    \n",
    "    df = df.drop_duplicates(subset = \"username\", keep = \"first\")\n",
    "    \n",
    "    df = df.dropna(subset = subset, how = \"all\")\n",
    "\n",
    "    df.to_parquet(\"data/parquet/removed_duplicate_df.parquet\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a59b228-47bb-48aa-92b3-5f170636dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_type(df):\n",
    "    \n",
    "    df[\"no_of_rooms\"] = df[\"no_of_rooms\"].astype(\"category\")\n",
    "    \n",
    "    df[\"rent_cost\"] = df[\"rent_cost\"].astype(\"float\")\n",
    "    \n",
    "    df[\"energy_bill\"] = df[\"energy_bill\"].astype(\"float\")\n",
    "    \n",
    "    df[\"water\"] = df[\"water\"].astype(\"float\")\n",
    "    \n",
    "    df[\"council_tax\"] = df[\"council_tax\"].astype(\"float\")\n",
    "    \n",
    "    df[\"groceries\"] = df[\"groceries\"].astype(\"float\")\n",
    "    \n",
    "    df[\"clothing\"] = df[\"clothing\"].astype(\"float\")\n",
    "\n",
    "    clean_df = df[[\"county\", \"no_of_rooms\", \"rent_cost\", \"energy_bill\", \"water\", \"council_tax\", \"groceries\", \"clothing\"]]\n",
    "\n",
    "    clean_df = clean_df.reset_index()\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bd33388-67f9-439b-9ab4-3ce375b205ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    df.to_parquet(\"data/parquet/clean_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ac194b5-e893-4d1f-9053-d3740309cbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data pipeline at 2024-08-16 05:58:20\n",
      "_____________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'twikit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: Authenticate the twitter API and perform query search\u001b[39;00m\n\u001b[0;32m      5\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m extract_tweets()\n\u001b[0;32m      7\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted Step 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 58\u001b[0m, in \u001b[0;36mextract_tweets\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m     keys \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Creating a client instance\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m client \u001b[38;5;241m=\u001b[39m twikit\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mClient(language \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-US\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Defining an async function to handle the async login\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'twikit' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Starting data pipeline at {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\")\n",
    "print(\"_____________________________________________\")\n",
    "\n",
    "# Step 1: Authenticate the twitter API and perform query search\n",
    "# t0 = time.time()\n",
    "# extract_tweets()\n",
    "# t1 = time.time()\n",
    "# print(f\"Completed Step 1\")\n",
    "# print(f\"---> Process completed in {str(t1 - t0)} seconds \\n\")\n",
    "\n",
    "\n",
    "# Step 2: Merge the data files from step 1\n",
    "t0 = time.time()\n",
    "merge_csv()\n",
    "t1 = time.time()\n",
    "print(f\"Completed Step 2\")\n",
    "print(f\"---> Storage files merged in {str(t1 - t0)} seconds \\n\")\n",
    "\n",
    "\n",
    "# Step 3: Convert the file to a DataFrame \n",
    "t0 = time.time()\n",
    "df = extract_data()\n",
    "t1 = time.time()\n",
    "print(f\"Completed Step 3\")\n",
    "print(f\"---> Extracted data with regex in {str(t1 - t0)} seconds \\n\")\n",
    "\n",
    "\n",
    "# Step 4: Convert data types\n",
    "t0 = time.time()\n",
    "df = string_to_float(df)\n",
    "t1 = time.time()\n",
    "print(f\"Completed Step 4\")\n",
    "print(f\"---> Converted to appropriate data types in {str(t1 - t0)} seconds \\n\")\n",
    "\n",
    "\n",
    "# Step 5: Replace null values\n",
    "t0 = time.time()\n",
    "df = populate_df(df)\n",
    "t1 = time.time()\n",
    "print(f\"Completed Step 5\")\n",
    "print(f\"---> Null values replaced in {str(t1 - t0)} seconds \\n\")\n",
    "\n",
    "\n",
    "# Step 6: Remove null values and duplicates\n",
    "t0 = time.time()\n",
    "df = remove_na_dup(df)\n",
    "t1 = time.time()\n",
    "print(f\"Completed Step 6\")\n",
    "print(f\"---> Removed null and duplicate values in {str(t1 - t0)} seconds \\n\")\n",
    "\n",
    "\n",
    "# Step 7: Reconvert data types for persistence \n",
    "t0 = time.time()\n",
    "df = convert_type(df)\n",
    "t1 = time.time()\n",
    "print(f\"Completed Step 7\")\n",
    "print(f\"---> Converted data types in {str(t1 - t0)} seconds \\n\")\n",
    "\n",
    "\n",
    "# Step 8: Save the DataFrame to file\n",
    "t0 = time.time()\n",
    "load_data(df)\n",
    "t1 = time.time()\n",
    "print(f\"Completed Step 8\")\n",
    "print(f\"---> Saved DataFrame to file in {str(t1 - t0)} seconds \\n\")\n",
    "\n",
    "\n",
    "# Step 9: Preview DataFrame\n",
    "t0 = time.time()\n",
    "clean_df = pd.read_parquet(\"data/parquet/clean_df.parquet\", engine = \"pyarrow\")\n",
    "print(clean_df.head())\n",
    "t1 = time.time()\n",
    "print(f\"Completed Step 9\")\n",
    "print(f\"---> Loaded and previewed DataFrame in {str(t1 - t0)} seconds \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
